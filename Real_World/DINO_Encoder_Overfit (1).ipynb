{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MEI5NI8Vd1Z",
        "outputId": "bf144dd7-36cf-4392-93f4-e3f2c5bf7634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g-hwKwMMX9Dc",
        "outputId": "7deea7ee-573c-4479-fd0f-2bc7b0491a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GroundingDINO'...\n",
            "remote: Enumerating objects: 463, done.\u001b[K\n",
            "remote: Counting objects: 100% (240/240), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 463 (delta 176), reused 137 (delta 137), pack-reused 223 (from 1)\u001b[K\n",
            "Receiving objects: 100% (463/463), 12.87 MiB | 9.23 MiB/s, done.\n",
            "Resolving deltas: 100% (241/241), done.\n",
            "/content/GroundingDINO\n",
            "Obtaining file:///content/GroundingDINO\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (0.20.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (4.48.3)\n",
            "Collecting addict (from groundingdino==0.1.0)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting yapf (from groundingdino==0.1.0)\n",
            "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (1.0.14)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (4.11.0.86)\n",
            "Collecting supervision>=0.22.0 (from groundingdino==0.1.0)\n",
            "  Downloading supervision-0.25.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from groundingdino==0.1.0) (2.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (3.10.0)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.11/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm->groundingdino==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->groundingdino==0.1.0) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->groundingdino==0.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->groundingdino==0.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->groundingdino==0.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->groundingdino==0.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->groundingdino==0.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->groundingdino==0.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->groundingdino==0.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->groundingdino==0.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->groundingdino==0.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->groundingdino==0.1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->groundingdino==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->groundingdino==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->groundingdino==0.1.0) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->groundingdino==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->groundingdino==0.1.0) (0.21.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->groundingdino==0.1.0) (4.3.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->supervision>=0.22.0->groundingdino==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->supervision>=0.22.0->groundingdino==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->supervision>=0.22.0->groundingdino==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->supervision>=0.22.0->groundingdino==0.1.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->groundingdino==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision>=0.22.0->groundingdino==0.1.0) (1.17.0)\n",
            "Downloading supervision-0.25.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: addict, yapf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, supervision, nvidia-cusolver-cu12, groundingdino\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Running setup.py develop for groundingdino\n",
            "Successfully installed addict-2.4.0 groundingdino-0.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 supervision-0.25.1 yapf-0.43.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd GroundingDINO\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eCBGrxkmX-e6",
        "outputId": "ebf7875b-8aa0-46f0-bc0c-d92ef13f39b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-21 19:32:34--  https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/611591640/f221e500-c2fc-4fd3-b84e-8ad92a6923f3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250221%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250221T193234Z&X-Amz-Expires=300&X-Amz-Signature=178a4ea4859b8472691849c00e1f5c946991ab2ff5b0340475d575eb9b629f2d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dgroundingdino_swint_ogc.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-02-21 19:32:35--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/611591640/f221e500-c2fc-4fd3-b84e-8ad92a6923f3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250221%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250221T193234Z&X-Amz-Expires=300&X-Amz-Signature=178a4ea4859b8472691849c00e1f5c946991ab2ff5b0340475d575eb9b629f2d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dgroundingdino_swint_ogc.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 693997677 (662M) [application/octet-stream]\n",
            "Saving to: ‘weights/groundingdino_swint_ogc.pth’\n",
            "\n",
            "weights/groundingdi 100%[===================>] 661.85M  39.0MB/s    in 17s     \n",
            "\n",
            "2025-02-21 19:32:53 (38.7 MB/s) - ‘weights/groundingdino_swint_ogc.pth’ saved [693997677/693997677]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir weights\n",
        "!wget -O weights/groundingdino_swint_ogc.pth https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "o2eEc7IyVgS2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class SPOTDataLoader(Dataset):\n",
        "    def __init__(self, root_dir, labels_file, transform=None, preload=False):\n",
        "        print(\"Initializing SPOTDataLoader...\")\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.labels = np.load(labels_file)\n",
        "\n",
        "        self.preload = preload\n",
        "        self.cached_data = {}\n",
        "\n",
        "        if self.preload:\n",
        "            print(\"Preloading dataset into memory... This may take time.\")\n",
        "            for idx in range(len(self.labels)):\n",
        "                folder_name = format(idx, '05d')\n",
        "                folder_path = os.path.join(self.root_dir, folder_name)\n",
        "\n",
        "                input_images = []\n",
        "                for i in range(5):\n",
        "                    img_path = os.path.join(folder_path, f\"{i}.jpg\")\n",
        "                    image = Image.open(img_path).convert('RGB')\n",
        "                    if self.transform:\n",
        "                        image = self.transform(image)\n",
        "                    input_images.append(image)\n",
        "\n",
        "                goal_image_path = os.path.join(folder_path, f\"goal.jpg\")\n",
        "                goal_image = Image.open(goal_image_path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    goal_image = self.transform(goal_image)\n",
        "\n",
        "                self.cached_data[idx] = (torch.stack(input_images, dim=0), goal_image.unsqueeze(0), torch.tensor(self.labels[idx]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.preload:\n",
        "            return self.cached_data[idx]\n",
        "\n",
        "        folder_name = format(idx, '05d')\n",
        "        folder_path = os.path.join(self.root_dir, folder_name)\n",
        "\n",
        "        input_images = []\n",
        "        for i in range(5):\n",
        "            img_path = os.path.join(folder_path, f\"{i}.jpg\")\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            input_images.append(image)\n",
        "\n",
        "        goal_image_path = os.path.join(folder_path, f\"goal.jpg\")\n",
        "        goal_image = Image.open(goal_image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            goal_image = self.transform(goal_image)\n",
        "\n",
        "        return torch.stack(input_images, dim=0), goal_image.unsqueeze(0), torch.tensor(self.labels[idx])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ql1NFM33XO_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c3b7a8-6efa-438d-843f-06deaf59d1a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.models import build_model\n",
        "\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(CrossAttentionBlock, self).__init__()\n",
        "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "\n",
        "    def forward(self, query, key_value):\n",
        "        attn, _ = self.mha(query, key_value, key_value)\n",
        "        return attn\n",
        "\n",
        "class GroundingDinoFeatureExtractor(nn.Module):\n",
        "    def __init__(self, base_model, device='cuda'):\n",
        "        super(GroundingDinoFeatureExtractor, self).__init__()\n",
        "        self.model = base_model\n",
        "        self.device = device\n",
        "        self._features = None\n",
        "        self.hook_handle = self.model.transformer.encoder.layers[-1].register_forward_hook(self.hook_fn)\n",
        "\n",
        "    def hook_fn(self, module, input, output):\n",
        "        self._features = output\n",
        "\n",
        "    def forward(self, images, text_prompts):\n",
        "        images = images.to(self.device)\n",
        "        _ = self.model(images, captions=text_prompts)\n",
        "        return self._features\n",
        "\n",
        "class DINOCrossAttentionMLP(nn.Module):\n",
        "    def __init__(self, config_file, weight_file, num_cameras=5, embed_dim=256, device='cuda'):\n",
        "        super(DINOCrossAttentionMLP, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_cameras = num_cameras\n",
        "\n",
        "        cfg = SLConfig.fromfile(config_file)\n",
        "        base_model = build_model(cfg)\n",
        "        checkpoint = torch.load(weight_file, map_location=device)\n",
        "        state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
        "        state_dict = { (k[len(\"module.\"): ] if k.startswith(\"module.\") else k): v for k, v in state_dict.items() }\n",
        "        base_model.load_state_dict(state_dict, strict=False)\n",
        "        base_model.to(device)\n",
        "\n",
        "        for param in base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        base_model.eval()\n",
        "\n",
        "        self.feature_extractor = GroundingDinoFeatureExtractor(base_model, device=device)\n",
        "        self.cross_attention = CrossAttentionBlock(embed_dim, num_heads=8)\n",
        "\n",
        "        self.fc_layer1 = nn.Sequential(\n",
        "            nn.Linear(2 * embed_dim * num_cameras, 1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc_layer2 = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc_layer3 = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc_layer4 = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc_layer5 = nn.Linear(1024, 3)\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, current_images, goal_images, text_prompts):\n",
        "        if goal_images.size(1) == 1 and self.num_cameras > 1:\n",
        "            goal_images = goal_images.expand(-1, self.num_cameras, -1, -1, -1)\n",
        "\n",
        "        current_features_list = []\n",
        "        goal_features_list = []\n",
        "\n",
        "        for cam in range(self.num_cameras):\n",
        "            curr_img = current_images[:, cam, :, :, :]\n",
        "            goal_img = goal_images[:, cam, :, :, :]\n",
        "            curr_feat = self.feature_extractor(curr_img, text_prompts)\n",
        "            goal_feat = self.feature_extractor(goal_img, text_prompts)\n",
        "\n",
        "            if curr_feat is None or goal_feat is None:\n",
        "                print(f\"[Camera {cam}] Warning: Feature extraction returned None!\")\n",
        "                continue\n",
        "\n",
        "            curr_attn = curr_feat + self.cross_attention(curr_feat, goal_feat)\n",
        "            goal_attn = goal_feat + self.cross_attention(goal_feat, curr_feat)\n",
        "            curr_pool = curr_attn.mean(dim=1)\n",
        "            goal_pool = goal_attn.mean(dim=1)\n",
        "            current_features_list.append(curr_pool)\n",
        "            goal_features_list.append(goal_pool)\n",
        "\n",
        "        current_features = torch.cat(current_features_list, dim=1)\n",
        "        goal_features = torch.cat(goal_features_list, dim=1)\n",
        "        features = torch.cat([current_features, goal_features], dim=1)\n",
        "        x = self.fc_layer1(features)\n",
        "        x = self.fc_layer2(x)\n",
        "        x = self.fc_layer3(x)\n",
        "        x = self.fc_layer4(x)\n",
        "        output = self.fc_layer5(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "VADzV19TVqTP",
        "outputId": "b712aa49-9f59-41b4-e2a4-56d7080f72c8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'groundingdino'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a8e8e9de7d02>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSLConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgroundingdino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'groundingdino'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision import transforms\n",
        "from groundingdino.util.slconfig import SLConfig\n",
        "from groundingdino.models import build_model\n",
        "\n",
        "# === Paths ===\n",
        "SPLIT_DATA_PATH = '/content/drive/MyDrive/Spot_IL/Real World Dataset'\n",
        "LABEL_PATH = os.path.join(SPLIT_DATA_PATH, 'map01_01_train_5_1/labels.npy')\n",
        "TRAIN_PATH = os.path.join(SPLIT_DATA_PATH, 'map01_01_train_5_1')\n",
        "\n",
        "WEIGHT_PATH = os.path.join(SPLIT_DATA_PATH, 'weights/map01_01_DinoMlp')\n",
        "os.makedirs(WEIGHT_PATH, exist_ok=True)\n",
        "\n",
        "FIGURE_PATH = os.path.join(SPLIT_DATA_PATH, 'Results/map01_01_DinoMlp')\n",
        "os.makedirs(FIGURE_PATH, exist_ok=True)\n",
        "\n",
        "\n",
        "# === Data Transforms ===\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# === Device Setup ===\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {DEVICE}')\n",
        "\n",
        "\n",
        "# === Dataset ===\n",
        "full_dataset = SPOTDataLoader(\n",
        "    root_dir=TRAIN_PATH,\n",
        "    labels_file=LABEL_PATH,\n",
        "    transform=data_transforms,\n",
        "    preload=True\n",
        ")\n",
        "print(f\"Total training samples: {len(full_dataset)}\")\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_dataloader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "\n",
        "# === Hyperparameters & Loss ===\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 500\n",
        "START_EPOCH = 100  # Start training from the 101st epoch\n",
        "LOSS_SCALE = 1e3\n",
        "TOLERANCE = 1e-1\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "config_file = \"groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
        "weight_file = \"weights/groundingdino_swint_ogc.pth\"\n",
        "\n",
        "# === Model, Optimizer, and Scheduler ===\n",
        "model = DINOCrossAttentionMLP(\n",
        "    config_file=config_file,\n",
        "    weight_file=weight_file,\n",
        "    num_cameras=5,\n",
        "    embed_dim=256\n",
        ")\n",
        "model.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=10, verbose=True\n",
        ")\n",
        "\n",
        "# --- Loading weights from the 100th epoch ---\n",
        "checkpoint_path = os.path.join(WEIGHT_PATH, 'epoch_100.pth')\n",
        "if os.path.exists(checkpoint_path):\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
        "    print(f\"Loaded weights from {checkpoint_path}\")\n",
        "else:\n",
        "    print(f\"No checkpoint found at {checkpoint_path}, starting from scratch.\")\n",
        "\n",
        "training_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(START_EPOCH, NUM_EPOCHS + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for current_images, goal_images, labels in train_dataloader:\n",
        "        current_images = current_images.to(DEVICE)\n",
        "        goal_images = goal_images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        text_prompts = [\"green chair.\" for _ in range(current_images.size(0))]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(current_images, goal_images, text_prompts)\n",
        "\n",
        "        # Computing loss\n",
        "        loss = loss_fn(output, labels.float()) * LOSS_SCALE\n",
        "        loss.backward()\n",
        "\n",
        "        # Applying gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Computing accuracy\n",
        "        errors = torch.norm(output - labels.float(), dim=1, p=2)\n",
        "        train_correct += (errors < TOLERANCE).sum().item()\n",
        "        train_total += errors.numel()\n",
        "\n",
        "    # Loss and accuracy per epoch\n",
        "    epoch_loss = running_loss / len(train_dataloader)\n",
        "    train_accuracy = (train_correct / train_total) * 100\n",
        "    training_losses.append(epoch_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{NUM_EPOCHS} -- Training Loss: {epoch_loss:.6f} -- Training Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Adjusting learning rate if model is not improving\n",
        "    scheduler.step(epoch_loss)\n",
        "\n",
        "    # --- Saving Weights Every 25 Epochs ---\n",
        "    if epoch % 25 == 0:\n",
        "        weight_file = os.path.join(WEIGHT_PATH, f'epoch_{epoch}.pth')\n",
        "        torch.save(model.state_dict(), weight_file)\n",
        "        print(f\"Weights saved at epoch {epoch}\")\n",
        "\n",
        "# === Final Model Save ===\n",
        "final_weight_file = os.path.join(WEIGHT_PATH, f'final_epoch_{NUM_EPOCHS}.pth')\n",
        "torch.save(model.state_dict(), final_weight_file)\n",
        "print(\"Training complete. Final model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M6DrrOdwVjq2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "WEIGHT_SAVING_STEP = 10\n",
        "DPI = 120\n",
        "FIGURE_SIZE_PIXEL = [2490, 1490]\n",
        "FIGURE_SIZE = [fsp / DPI for fsp in FIGURE_SIZE_PIXEL]\n",
        "\n",
        "def plot_graph(training_losses, train_accuracies, figure_path=None,\n",
        "               loss_filename='Training_loss.png', accuracy_filename='Training_accuracy.png',\n",
        "               start_plot=0, end_plot=None):\n",
        "    if end_plot is None or end_plot > len(training_losses):\n",
        "        end_plot = len(training_losses)\n",
        "\n",
        "    epochs = range(start_plot + 1, end_plot + 1)\n",
        "\n",
        "    # ===== Training Loss =====\n",
        "    plt.figure(figsize=FIGURE_SIZE, dpi=DPI)\n",
        "    plt.scatter(epochs, training_losses[start_plot:end_plot], color='blue', label='Training Loss')\n",
        "    plt.plot(epochs, training_losses[start_plot:end_plot], color='cyan', linestyle='-', label='Loss Trend')\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss (scaled)\")\n",
        "    plt.legend()\n",
        "\n",
        "    lowest_loss = min(training_losses[start_plot:end_plot])\n",
        "    for i, loss in enumerate(training_losses[start_plot:end_plot], start=start_plot+1):\n",
        "        if (i % WEIGHT_SAVING_STEP == 0) or (i == end_plot):\n",
        "            plt.annotate(str(round(loss, 6)), xy=(i, loss))\n",
        "\n",
        "    plt.text(0, plt.gca().get_ylim()[1], f'Lowest Loss: {lowest_loss:.6f}')\n",
        "\n",
        "    if figure_path is not None:\n",
        "        plt.savefig(os.path.join(figure_path, loss_filename))\n",
        "    plt.show()\n",
        "\n",
        "    # ===== Training Accuracy =====\n",
        "    plt.figure(figsize=FIGURE_SIZE, dpi=DPI)\n",
        "    plt.plot(epochs, train_accuracies[start_plot:end_plot], color='green', linestyle='-', marker='o',\n",
        "             label='Training Accuracy')\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.legend()\n",
        "\n",
        "    for i, acc in enumerate(train_accuracies[start_plot:end_plot], start=start_plot+1):\n",
        "        if (i % WEIGHT_SAVING_STEP == 0) or (i == end_plot):\n",
        "            plt.annotate(f\"{round(acc, 2)}\", xy=(i, acc))\n",
        "\n",
        "    if figure_path is not None:\n",
        "        plt.savefig(os.path.join(figure_path, accuracy_filename))\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(training_losses, train_accuracies, FIGURE_PATH,\n",
        "               loss_filename='Training_loss.png', accuracy_filename='Training_accuracy.png',\n",
        "               start_plot=0, end_plot=500)"
      ],
      "metadata": {
        "id": "b7OQ8crYML5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_history = {\n",
        "    'losses': training_losses,\n",
        "    'accuracies': train_accuracies\n",
        "}\n",
        "\n",
        "history_file = os.path.join(FIGURE_PATH, 'dinomlp_training_history.pth')\n",
        "torch.save(training_history, history_file)\n",
        "print(f\"Training history saved to {history_file}\")"
      ],
      "metadata": {
        "id": "fpDO7rDk_gAx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}