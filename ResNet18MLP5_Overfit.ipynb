{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "9fsnqyvcCPYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd52b830-f404-48b1-f7b1-a1484b1b9c4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "class SPOTDataLoader(Dataset):\n",
        "    def __init__(self, root_dir, labels_file, transform=None):\n",
        "        print(\"Initializing SPOTDataLoader...\")\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.labels = np.load(labels_file)\n",
        "        # print(f\"Loaded labels from {labels_file} with shape: {self.labels.shape}\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda = True\n",
        "            # print(\"CUDA is available. Using GPU.\")\n",
        "        else:\n",
        "            self.cuda = False\n",
        "            # print(\"CUDA is not available. Using CPU.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        length = self.labels.shape[0]\n",
        "        # print(f\"Dataset length: {length}\")\n",
        "        return length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(f\"\\nFetching data for index: {idx}\")\n",
        "        folder_name = format(idx, '05d')\n",
        "        folder_path = os.path.join(self.root_dir, folder_name)\n",
        "        # print(f\"Constructed folder path: {folder_path}\")\n",
        "\n",
        "        input_images = []\n",
        "        for i in range(5):\n",
        "            input_image_path = os.path.join(folder_path, f\"{i}.jpg\")\n",
        "            # print(f\"Loading input image {i} from: {input_image_path}\")\n",
        "            image = Image.open(input_image_path).convert('RGB')\n",
        "            # print(f\"Loaded input image {i} with size: {image.size}\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "                #print(f\"Applied transform to input image {i}.\")\n",
        "            input_images.append(image)\n",
        "\n",
        "        goal_images = []\n",
        "        for i in range(1):\n",
        "            goal_image_path = os.path.join(folder_path, f\"goal.jpg\")\n",
        "            # print(f\"Loading goal image {i} from: {goal_image_path}\")\n",
        "            image = Image.open(goal_image_path).convert('RGB')\n",
        "            # print(f\"Loaded goal image {i} with size: {image.size}\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "                # print(f\"Applied transform to goal image {i}.\")\n",
        "            goal_images.append(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        # print(f\"Label for index {idx}: {label}\")\n",
        "\n",
        "        if self.cuda:\n",
        "            input_images_tensor = torch.stack(input_images, dim=0).cuda()\n",
        "            goal_images_tensor = torch.stack(goal_images, dim=0).cuda()\n",
        "            label_tensor = torch.tensor(label).cuda()\n",
        "            # print(\"Moved input images, goal images, and label tensor to GPU.\")\n",
        "        else:\n",
        "            input_images_tensor = torch.stack(input_images, dim=0)\n",
        "            goal_images_tensor = torch.stack(goal_images, dim=0)\n",
        "            label_tensor = torch.tensor(label)\n",
        "            # print(\"Using CPU tensors for input images, goal images, and label tensor.\")\n",
        "\n",
        "        # print(f\"Input images tensor shape: {input_images_tensor.shape}\")\n",
        "        # print(f\"Goal images tensor shape: {goal_images_tensor.shape}\")\n",
        "        # print(f\"Label tensor: {label_tensor}\")\n",
        "\n",
        "        return input_images_tensor, goal_images_tensor, label_tensor"
      ],
      "metadata": {
        "id": "w2HGxd84cPgn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(CrossAttentionBlock, self).__init__()\n",
        "        self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def forward(self, query, key_value):\n",
        "        # query and key_value: (B, C, H, W)\n",
        "        B, C, H, W = query.shape\n",
        "\n",
        "        # (H*W, B, C)\n",
        "        query_flat = query.view(B, C, -1).permute(2, 0, 1)\n",
        "\n",
        "        key_value_flat = key_value.view(B, C, -1).permute(2, 0, 1)\n",
        "        attn_output, _ = self.mha(query_flat, key_value_flat, key_value_flat)\n",
        "\n",
        "        # (B, C, H, W)\n",
        "        attn_output = attn_output.permute(1, 2, 0).view(B, C, H, W)\n",
        "        return attn_output\n",
        "\n",
        "class SharedResNet18MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SharedResNet18MLP, self).__init__()\n",
        "        # Shared ResNet18 trunk (excluding the last 2 layers)\n",
        "        base_resnet = resnet18(weights=None)\n",
        "        self.shared_trunk = nn.Sequential(*list(base_resnet.children())[:-2])\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        num_trunk_channels = 512\n",
        "        self.num_cameras = 5\n",
        "\n",
        "        # Camera-specific heads for current images\n",
        "        self.current_heads = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(num_trunk_channels, num_trunk_channels, kernel_size=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ) for _ in range(self.num_cameras)\n",
        "        ])\n",
        "\n",
        "        # Camera-specific heads for the goal image\n",
        "        self.goal_heads = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(num_trunk_channels, num_trunk_channels, kernel_size=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ) for _ in range(self.num_cameras)\n",
        "        ])\n",
        "\n",
        "        # Cross-attention block shared across cameras\n",
        "        self.cross_attention = CrossAttentionBlock(embed_dim=num_trunk_channels, num_heads=8)\n",
        "\n",
        "        # Fully connected layers.\n",
        "        # Input feature dimension: 5 cameras * 2 (current + goal) * 512 = 5120.\n",
        "        self.fc_layer1 = nn.Sequential(\n",
        "            nn.Linear(5120, 1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_layer2 = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_layer3 = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_layer4 = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Final output layer produces 3 regression outputs.\n",
        "        self.fc_layer5 = nn.Linear(1024, 3)\n",
        "\n",
        "    def forward(self, current_images, goal_image):\n",
        "        batch_size = current_images.size(0)\n",
        "        current_features_list = []\n",
        "        goal_features_list = []\n",
        "\n",
        "        # Processing the goal image once through the shared trunk.\n",
        "        goal_trunk_feat = self.shared_trunk(goal_image)  # (B, 512, H', W')\n",
        "\n",
        "        for cam_idx in range(self.num_cameras):\n",
        "            # Processing current image for camera cam_idx.\n",
        "            curr = current_images[:, cam_idx, :, :, :]  # (B, C, H, W)\n",
        "            curr_feat = self.shared_trunk(curr)         # (B, 512, H', W')\n",
        "            curr_feat = self.current_heads[cam_idx](curr_feat)\n",
        "\n",
        "            # Processing the same goal image\n",
        "            goal_feat = self.goal_heads[cam_idx](goal_trunk_feat)\n",
        "\n",
        "            # Applying cross-attention in both directions.\n",
        "            curr_attended = curr_feat + self.cross_attention(curr_feat, goal_feat)\n",
        "            goal_attended = goal_feat + self.cross_attention(goal_feat, curr_feat)\n",
        "\n",
        "            # Global pooling.\n",
        "            curr_pooled = self.global_pool(curr_attended).view(batch_size, -1)  # (B, 512)\n",
        "            goal_pooled = self.global_pool(goal_attended).view(batch_size, -1)  # (B, 512)\n",
        "\n",
        "            current_features_list.append(curr_pooled)\n",
        "            goal_features_list.append(goal_pooled)\n",
        "\n",
        "        # Concatenating features from all cameras.\n",
        "        current_features = torch.cat(current_features_list, dim=1)  # (B, 5*512)\n",
        "        goal_features = torch.cat(goal_features_list, dim=1)        # (B, 5*512)\n",
        "        features = torch.cat([current_features, goal_features], dim=1)  # (B, 5120)\n",
        "\n",
        "        # Fully connected layers.\n",
        "        x = self.fc_layer1(features)\n",
        "        x = self.fc_layer2(x)\n",
        "        x = self.fc_layer3(x)\n",
        "        x = self.fc_layer4(x)\n",
        "        output = self.fc_layer5(x)  # (B, 3)\n",
        "        return output"
      ],
      "metadata": {
        "id": "bh5VgRY0dhFb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "WEIGHT_SAVING_STEP = 10\n",
        "DPI = 120\n",
        "FIGURE_SIZE_PIXEL = [2490, 1490]\n",
        "FIGURE_SIZE = [fsp / DPI for fsp in FIGURE_SIZE_PIXEL]\n",
        "\n",
        "def plot_graph(training_losses, train_accuracies, figure_path=None, start_plot=0, end_plot=None):\n",
        "    if end_plot is None or end_plot > len(training_losses):\n",
        "        end_plot = len(training_losses)\n",
        "\n",
        "    epochs = range(start_plot + 1, end_plot + 1)\n",
        "\n",
        "    # =====Training Loss =====\n",
        "    plt.figure(figsize=FIGURE_SIZE, dpi=DPI)\n",
        "    plt.scatter(epochs, training_losses[start_plot:end_plot], color='blue', label='Training Loss')\n",
        "    plt.plot(epochs, training_losses[start_plot:end_plot], color='cyan', linestyle='-', label='Loss Trend')\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss (scaled)\")\n",
        "    plt.legend()\n",
        "\n",
        "    lowest_loss = min(training_losses[start_plot:end_plot])\n",
        "    for i, loss in enumerate(training_losses[start_plot:end_plot], start=start_plot+1):\n",
        "        if (i % WEIGHT_SAVING_STEP == 0) or (i == end_plot):\n",
        "            plt.annotate(str(round(loss, 6)), xy=(i, loss))\n",
        "\n",
        "    plt.text(0, plt.gca().get_ylim()[1], f'Lowest Loss: {lowest_loss:.6f}')\n",
        "\n",
        "    if figure_path is not None:\n",
        "        plt.savefig(os.path.join(figure_path, 'Training_loss.png'))\n",
        "    plt.show()\n",
        "\n",
        "    # ===== Training Accuracy =====\n",
        "    plt.figure(figsize=FIGURE_SIZE, dpi=DPI)\n",
        "    plt.plot(epochs, train_accuracies[start_plot:end_plot], color='green', linestyle='-', marker='o',\n",
        "             label='Training Accuracy')\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.legend()\n",
        "\n",
        "    for i, acc in enumerate(train_accuracies[start_plot:end_plot], start=start_plot+1):\n",
        "        if (i % WEIGHT_SAVING_STEP == 0) or (i == end_plot):\n",
        "            plt.annotate(f\"{round(acc, 2)}\", xy=(i, acc))\n",
        "\n",
        "    if figure_path is not None:\n",
        "        plt.savefig(os.path.join(figure_path, 'Training_accuracy.png'))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "_mCFctrgfG2W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# === Paths ===\n",
        "SPLIT_DATA_PATH = '/content/drive/MyDrive/Spot_IL/Real World Dataset'\n",
        "LABEL_PATH = os.path.join(SPLIT_DATA_PATH, 'map01_01_train_5_1/labels.npy')\n",
        "TRAIN_PATH = os.path.join(SPLIT_DATA_PATH, 'map01_01_train_5_1')\n",
        "\n",
        "WEIGHT_PATH = os.path.join(SPLIT_DATA_PATH, 'weights/map01_01_Resnet18Mlp')\n",
        "os.makedirs(WEIGHT_PATH, exist_ok=True)\n",
        "\n",
        "FIGURE_PATH = os.path.join(SPLIT_DATA_PATH, 'Results/map01_01_Resnet18Mlp')\n",
        "os.makedirs(FIGURE_PATH, exist_ok=True)\n",
        "\n",
        "# === Data Transforms ===\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# === Device Setup ===\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {DEVICE}')\n",
        "\n",
        "# === Dataset (No Split) ===\n",
        "full_dataset = SPOTDataLoader(\n",
        "    root_dir=TRAIN_PATH,\n",
        "    labels_file=LABEL_PATH,\n",
        "    transform=data_transforms\n",
        ")\n",
        "\n",
        "# Using entire dataset for training\n",
        "train_dataset = full_dataset\n",
        "print(f'Training samples: {len(train_dataset)}')\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "\n",
        "# === Hyperparameters & Loss ===\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 500\n",
        "LOSS_SCALE = 1e3\n",
        "TOLERANCE = 1e-1\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "# === Model & Optimizer ===\n",
        "model = SharedResNet18MLP().to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
        "\n",
        "# === Training Loop ===\n",
        "training_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # --- Training ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for current_images, goal_images, labels in train_dataloader:\n",
        "        goal_images = goal_images.squeeze(1)\n",
        "        current_images = current_images.to(DEVICE)\n",
        "        goal_images = goal_images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(current_images, goal_images)  # (B, 3)\n",
        "        loss = loss_fn(output, labels.float()) * LOSS_SCALE\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataloader)\n",
        "    training_losses.append(epoch_loss)\n",
        "    print(f\"Epoch {epoch}/{NUM_EPOCHS} -- Training Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    # --- Training Accuracy ---\n",
        "    model.eval()\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    with torch.no_grad():\n",
        "        for current_images, goal_images, labels in train_dataloader:\n",
        "            goal_images = goal_images.squeeze(1)\n",
        "            current_images = current_images.to(DEVICE)\n",
        "            goal_images = goal_images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            output = model(current_images, goal_images)  # (B, 3)\n",
        "            for i in range(output.size(0)):\n",
        "                error = torch.norm(output[i] - labels[i].float(), p=2).item()\n",
        "                train_total += 1\n",
        "                if error < TOLERANCE:\n",
        "                    train_correct += 1\n",
        "\n",
        "    train_accuracy = (train_correct / train_total) * 100\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    print(f\"Epoch {epoch}/{NUM_EPOCHS} -- Training Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # --- Save Weights Every 50 Epochs ---\n",
        "    if epoch % 50 == 0:\n",
        "        weight_file = os.path.join(WEIGHT_PATH, f'epoch_{epoch}.pth')\n",
        "        torch.save(model.state_dict(), weight_file)\n",
        "        print(f\"Weights saved at epoch {epoch}\")\n",
        "\n",
        "    # scheduler.step()\n",
        "\n",
        "# === Final Model Save ===\n",
        "final_weight_file = os.path.join(WEIGHT_PATH, f'final_epoch_{NUM_EPOCHS}.pth')\n",
        "torch.save(model.state_dict(), final_weight_file)\n",
        "print(\"Training complete. Final model saved.\")\n"
      ],
      "metadata": {
        "id": "nCL1zckNd6Hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ec7b6b-9880-44f7-da26-cc2928080d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Initializing SPOTDataLoader...\n",
            "Training samples: 1993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(training_losses, train_accuracies, FIGURE_PATH, start_plot=0, end_plot=NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "oiBxld6dNtMg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}