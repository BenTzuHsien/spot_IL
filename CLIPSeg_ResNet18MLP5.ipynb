{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPpWtyXUDnC3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "class FiveResNet18MLP5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FiveResNet18MLP5, self).__init__()\n",
        "\n",
        "        # Current Images Set\n",
        "        # ResNet1\n",
        "        self.current_resnet1 = resnet18(weights=None)\n",
        "        self.current_resnet1 = nn.Sequential(*list(self.current_resnet1.children())[:-1])\n",
        "        # ResNet2\n",
        "        self.current_resnet2 = resnet18(weights=None)\n",
        "        self.current_resnet2 = nn.Sequential(*list(self.current_resnet2.children())[:-1])\n",
        "        # ResNet3\n",
        "        self.current_resnet3 = resnet18(weights=None)\n",
        "        self.current_resnet3 = nn.Sequential(*list(self.current_resnet3.children())[:-1])\n",
        "        # ResNet4\n",
        "        self.current_resnet4 = resnet18(weights=None)\n",
        "        self.current_resnet4 = nn.Sequential(*list(self.current_resnet4.children())[:-1])\n",
        "        # ResNet5\n",
        "        self.current_resnet5 = resnet18(weights=None)\n",
        "        self.current_resnet5 = nn.Sequential(*list(self.current_resnet5.children())[:-1])\n",
        "\n",
        "        # Goal Images Set\n",
        "        # ResNet1\n",
        "        self.goal_resnet1 = resnet18(weights=None)\n",
        "        self.goal_resnet1 = nn.Sequential(*list(self.goal_resnet1.children())[:-1])\n",
        "        # ResNet2\n",
        "        self.goal_resnet2 = resnet18(weights=None)\n",
        "        self.goal_resnet2 = nn.Sequential(*list(self.goal_resnet2.children())[:-1])\n",
        "        # ResNet3\n",
        "        self.goal_resnet3 = resnet18(weights=None)\n",
        "        self.goal_resnet3 = nn.Sequential(*list(self.goal_resnet3.children())[:-1])\n",
        "        # ResNet4\n",
        "        self.goal_resnet4 = resnet18(weights=None)\n",
        "        self.goal_resnet4 = nn.Sequential(*list(self.goal_resnet4.children())[:-1])\n",
        "        # ResNet5\n",
        "        self.goal_resnet5 = resnet18(weights=None)\n",
        "        self.goal_resnet5 = nn.Sequential(*list(self.goal_resnet5.children())[:-1])\n",
        "\n",
        "        # MLP Layers\n",
        "        self.fc_layer1 = nn.Sequential(\n",
        "            nn.Linear(5120, 1024),\n",
        "            nn.ReLU())\n",
        "        self.fc_layer2 = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU())\n",
        "        self.fc_layer3 = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU())\n",
        "        self.fc_layer4 = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU())\n",
        "        self.fc_layer5 = nn.Linear(1024, 1)\n",
        "\n",
        "    def forward(self, current_images, goal_images):\n",
        "\n",
        "        # Forward pass through ResNet\n",
        "        current_embedding1 = self.current_resnet1(current_images[:, 0, :, :])\n",
        "        current_embedding1 = torch.flatten(current_embedding1, start_dim=1)\n",
        "        current_embedding2 = self.current_resnet2(current_images[:, 1, :, :])\n",
        "        current_embedding2 = torch.flatten(current_embedding2, start_dim=1)\n",
        "        current_embedding3 = self.current_resnet3(current_images[:, 2, :, :])\n",
        "        current_embedding3 = torch.flatten(current_embedding3, start_dim=1)\n",
        "        current_embedding4 = self.current_resnet4(current_images[:, 3, :, :])\n",
        "        current_embedding4 = torch.flatten(current_embedding4, start_dim=1)\n",
        "        current_embedding5 = self.current_resnet5(current_images[:, 4, :, :])\n",
        "        current_embedding5 = torch.flatten(current_embedding5, start_dim=1)\n",
        "\n",
        "        # Forward pass through ResNet\n",
        "        goal_embedding1 = self.goal_resnet1(goal_images[:, 0, :, :])\n",
        "        goal_embedding1 = torch.flatten(goal_embedding1, start_dim=1)\n",
        "        goal_embedding2 = self.goal_resnet2(goal_images[:, 1, :, :])\n",
        "        goal_embedding2 = torch.flatten(goal_embedding2, start_dim=1)\n",
        "        goal_embedding3 = self.goal_resnet3(goal_images[:, 2, :, :])\n",
        "        goal_embedding3 = torch.flatten(goal_embedding3, start_dim=1)\n",
        "        goal_embedding4 = self.goal_resnet4(goal_images[:, 3, :, :])\n",
        "        goal_embedding4 = torch.flatten(goal_embedding4, start_dim=1)\n",
        "        goal_embedding5 = self.goal_resnet5(goal_images[:, 4, :, :])\n",
        "        goal_embedding5 = torch.flatten(goal_embedding5, start_dim=1)\n",
        "\n",
        "        # Concatenate the features\n",
        "        current_features = torch.cat((current_embedding1, current_embedding2, current_embedding3, current_embedding4, current_embedding5), dim=1)\n",
        "        goal_features = torch.cat((goal_embedding1, goal_embedding2, goal_embedding3, goal_embedding4, goal_embedding5), dim=1)\n",
        "        features = torch.cat([current_features, goal_features], dim=1)\n",
        "\n",
        "        # Forward pass through the fully connected layers\n",
        "        output1 = self.fc_layer1(features)\n",
        "        output2 = self.fc_layer2(output1)\n",
        "        output3 = self.fc_layer3(output2)\n",
        "        output4 = self.fc_layer4(output3)\n",
        "        output = self.fc_layer5(output4)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class SPOTDataLoader(Dataset):\n",
        "    def __init__(self, root_dir, goal_folder, labels_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.goal_folder = goal_folder\n",
        "        self.transform = transform\n",
        "        self.labels = np.load(labels_file)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda = True\n",
        "        else:\n",
        "            self.cuda = False\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        folder_name = format(idx, '05d')\n",
        "        folder_path = os.path.join(self.root_dir, folder_name)\n",
        "\n",
        "        input_images = []\n",
        "        for i in range(5):\n",
        "            input_image_path = os.path.join(folder_path, f\"{i}.png\")\n",
        "            image = Image.open(input_image_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            input_images.append(image)\n",
        "\n",
        "        goal_images = []\n",
        "        goal_folder_path = os.path.join(self.root_dir, self.goal_folder)\n",
        "        for i in range(5):\n",
        "            goal_image_path = os.path.join(goal_folder_path, f\"{i}.png\")\n",
        "            image = Image.open(goal_image_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            goal_images.append(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.cuda is True:\n",
        "            input_images = torch.stack(input_images, dim=0).cuda()\n",
        "            goal_images = torch.stack(goal_images, dim=0).cuda()\n",
        "            label_tensor = torch.tensor(label).cuda()\n",
        "        else:\n",
        "            input_images = torch.stack(input_images, dim=0)\n",
        "            goal_images = torch.stack(goal_images, dim=0)\n",
        "            label_tensor = torch.tensor(label)\n",
        "\n",
        "        return input_images, goal_images, label_tensor"
      ],
      "metadata": {
        "id": "MVg6p4B3EBTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a373b09-4dea-4974-cf2f-4d85225a3c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "DATASET_INITIAL_PATH = '/content/drive/MyDrive/Spot_IL/CLIPSeg_Mixed_Dataset_Train_Test'\n",
        "TRAIN_PATH = DATASET_INITIAL_PATH + '/train/'\n",
        "TEST_PATH = DATASET_INITIAL_PATH + '/test/'\n",
        "\n",
        "print(TEST_PATH)\n",
        "print(TRAIN_PATH)\n",
        "\n",
        "def quaternion_to_radians(quaternion):\n",
        "    qx, qy, qz, qw = quaternion[-4:]\n",
        "    yaw = np.arctan2(2 * (qw * qz + qx * qy), 1 - 2 * (qy**2 + qz**2))\n",
        "    return yaw\n",
        "\n",
        "def convert_labels_to_radians(path):\n",
        "    labels_path = os.path.join(path, 'labels.npy')\n",
        "    radians_labels_path = os.path.join(path, 'labels_radians.npy')\n",
        "    if os.path.exists(labels_path):\n",
        "        labels = np.load(labels_path)\n",
        "        radians_labels = [quaternion_to_radians(label) for label in labels]\n",
        "        np.save(radians_labels_path, radians_labels)\n",
        "        print(f\"Converted quaternions to radians and saved to {labels_path}\")\n",
        "\n",
        "convert_labels_to_radians(TRAIN_PATH)\n",
        "convert_labels_to_radians(TEST_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JlO1hpHGD4_",
        "outputId": "fae32be7-9b47-455b-adb2-cf6a968073c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Spot_IL/CLIPSeg_Mixed_Dataset_Train_Test/test/\n",
            "/content/drive/MyDrive/Spot_IL/CLIPSeg_Mixed_Dataset_Train_Test/train/\n",
            "Converted quaternions to radians and saved to /content/drive/MyDrive/Spot_IL/CLIPSeg_Mixed_Dataset_Train_Test/train/labels.npy\n",
            "Converted quaternions to radians and saved to /content/drive/MyDrive/Spot_IL/CLIPSeg_Mixed_Dataset_Train_Test/test/labels.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "WEIGHT_SAVING_STEP = 10\n",
        "DPI = 120\n",
        "FIGURE_SIZE_PIXEL = [2490, 1490]\n",
        "FIGURE_SIZE = [fsp / DPI for fsp in FIGURE_SIZE_PIXEL]\n",
        "\n",
        "def plot_graph(training_losses, accuracies, figure_path=None, fold=0, start_plot=0, end_plot=0):\n",
        "\n",
        "    if start_plot == end_plot:\n",
        "        return\n",
        "\n",
        "    # Fill with zero\n",
        "    for i in range(start_plot):\n",
        "        training_losses[i] = [0, 0]\n",
        "        accuracies[i] = [0, 0]\n",
        "\n",
        "    # Plot Training Loss\n",
        "    training_loss = [data[0] for data in training_losses]\n",
        "    average_loss = [data[1] for data in training_losses]\n",
        "\n",
        "    plt.figure(figsize=FIGURE_SIZE, dpi=DPI)\n",
        "    plt.scatter(range(start_plot + 1, end_plot + 1), training_loss[start_plot:], color='blue', label='Training Loss')\n",
        "    plt.plot(range(start_plot + 1, end_plot + 1), average_loss[start_plot:], color='cyan', linestyle='-', label='Average Training Loss')\n",
        "    plt.title(f\"Fold {fold} Training Loss\")\n",
        "    plt.xlabel(\"Epoches\")\n",
        "    plt.ylabel(\"Loss (1000 radians)\")\n",
        "    plt.legend()\n",
        "\n",
        "    lowest_loss = training_loss[0]\n",
        "    for i in range(end_plot):\n",
        "\n",
        "        if training_loss[i] < lowest_loss:\n",
        "            lowest_loss = training_loss[i]\n",
        "\n",
        "        if ((i + 1) % WEIGHT_SAVING_STEP) == 0:\n",
        "            plt.annotate(str(round(training_loss[i], 6)), xy=((i + 1), training_loss[i]))\n",
        "\n",
        "    plt.annotate(str(round(training_loss[end_plot - 1], 6)), xy=(end_plot, training_loss[end_plot - 1]))\n",
        "\n",
        "    plt.text(0, plt.gca().get_ylim()[1], f'Lowest Loss: {lowest_loss: .6f}')\n",
        "\n",
        "    if figure_path is not None:\n",
        "        plt.savefig(figure_path + f'Fold_{fold}_Training_loss.png')\n",
        "        plt.close()\n",
        "\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    train_accuracy = [data[0] for data in accuracies]\n",
        "    valid_accuracy = [data[1] for data in accuracies]\n",
        "\n",
        "    plt.figure(figsize=FIGURE_SIZE, dpi=DPI)\n",
        "    plt.plot(range(start_plot + 1, end_plot + 1), train_accuracy[start_plot:], color='blue', linestyle='-', marker='o', label='Training Accuracy')\n",
        "    plt.plot(range(start_plot + 1, end_plot + 1), valid_accuracy[start_plot:], color='orange', linestyle='-', marker='o', label='Validation Accuracy')\n",
        "    plt.title(f\"Fold {fold} Accuracy\")\n",
        "    plt.xlabel(\"Epoches\")\n",
        "    plt.ylabel(\"Acurracy (%)\")\n",
        "    plt.legend()\n",
        "\n",
        "    for i in range(end_plot):\n",
        "        if ((i + 1) % WEIGHT_SAVING_STEP) == 0:\n",
        "            plt.annotate(str(round(train_accuracy[i], 2)), xy=((i + 1), train_accuracy[i]))\n",
        "            plt.annotate(str(round(valid_accuracy[i], 2)), xy=((i + 1), valid_accuracy[i]))\n",
        "    plt.annotate(str(round(train_accuracy[end_plot - 1], 2)), xy=(end_plot, train_accuracy[end_plot - 1]))\n",
        "    plt.annotate(str(round(valid_accuracy[end_plot - 1], 2)), xy=(end_plot, valid_accuracy[end_plot - 1]))\n",
        "\n",
        "    if figure_path is not None:\n",
        "        plt.savefig(figure_path + f'Fold_{fold}_Accuracy.png')\n",
        "        plt.close()\n",
        "\n",
        "    else:\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "hL3ZVuSy9mqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch, os\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Setup Destination\n",
        "DATASET_NAME = 'mixed'\n",
        "DATASET_INITIAL_PATH = '/content/drive/MyDrive/Spot_IL/CLIPSeg_Mixed_Dataset_Train_Test'\n",
        "WEIGHT_FOLDER_NAME = 'lr1e-5_with_scaling'\n",
        "\n",
        "# Paths\n",
        "TRAIN_PATH = os.path.join(DATASET_INITIAL_PATH, 'train/')\n",
        "TEST_PATH = os.path.join(DATASET_INITIAL_PATH, 'test/')\n",
        "GOAL_PATH = os.path.join(DATASET_INITIAL_PATH, 'goal/goal_images/')\n",
        "LABEL_PATH = os.path.join(TRAIN_PATH, 'labels_radians.npy')\n",
        "\n",
        "print(TRAIN_PATH)\n",
        "print(TEST_PATH)\n",
        "print(GOAL_PATH)\n",
        "print(LABEL_PATH)\n",
        "\n",
        "# Output Paths\n",
        "WEIGHT_PATH = os.path.join(DATASET_INITIAL_PATH,'train', f'weights/Train_FiveResNet18MLP5_{DATASET_NAME}', WEIGHT_FOLDER_NAME)\n",
        "FIGURE_PATH = os.path.join(DATASET_INITIAL_PATH, 'train',f'Results/Train_FiveResNet18MLP5_{DATASET_NAME}', WEIGHT_FOLDER_NAME)\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(WEIGHT_PATH, exist_ok=True)\n",
        "os.makedirs(FIGURE_PATH, exist_ok=True)\n",
        "\n",
        "# KFold Parameters\n",
        "NUM_FOLD = 5\n",
        "k_fold = KFold(NUM_FOLD, shuffle=True)\n",
        "CONTINUE = [0] * NUM_FOLD   # Start from beginning, use 0\n",
        "\n",
        "# Preprocess for images\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    train_dataset = SPOTDataLoader(\n",
        "        root_dir = TRAIN_PATH,\n",
        "        goal_folder = GOAL_PATH,\n",
        "        labels_file = LABEL_PATH,\n",
        "        transform = data_transforms\n",
        "    )\n",
        "    DEVICE = 'cuda'\n",
        "    print('Cuda')\n",
        "\n",
        "else:\n",
        "    train_dataset = SPOTDataLoader(\n",
        "        root_dir = TRAIN_PATH,\n",
        "        goal_folder = GOAL_PATH,\n",
        "        labels_file = LABEL_PATH,\n",
        "        transform = data_transforms\n",
        "    )\n",
        "    DEVICE = 'cpu'\n",
        "    print('CPU')\n",
        "\n",
        "# Hyper Parameters\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "BATCH_SIZE = 2\n",
        "LEARNING_RATE = 1e-5\n",
        "\n",
        "# Training Parameters\n",
        "WEIGHT_SAVING_STEP = 10\n",
        "LOSS_SCALE = 1e3\n",
        "\n",
        "# Validation Parameter\n",
        "TOLERANCE = 1e-4\n",
        "\n",
        "# Saving Hyper Param\n",
        "hyper_params_path = WEIGHT_PATH + 'hyper_params'\n",
        "hyper_params = {'NUM_FOLD': NUM_FOLD, 'BATCH_SIZE': BATCH_SIZE, 'LEARNING_RATE': LEARNING_RATE, 'LOSS_SCALE': LOSS_SCALE, 'TOLERANCE': TOLERANCE}\n",
        "np.savez(hyper_params_path, **hyper_params)\n",
        "\n",
        "for fold, (train_ids, valid_ids) in enumerate(k_fold.split(train_dataset)):\n",
        "    if fold < 2:\n",
        "        continue\n",
        "\n",
        "    print(f'FOLD {fold}')\n",
        "    fold_path = WEIGHT_PATH + 'fold_' + str(fold) + '/'\n",
        "    if not os.path.exists(fold_path):\n",
        "        os.mkdir(fold_path)\n",
        "\n",
        "    # Setup Model\n",
        "    model = FiveResNet18MLP5().to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Tracking Parameters\n",
        "    epoch = CONTINUE[fold] + 1\n",
        "    training_loss = 1e6\n",
        "    training_total_loss = 0\n",
        "    training_losses = []   #[training_loss training_average_loss]\n",
        "    tracking_losses_path = fold_path + 'training_losses.npy'\n",
        "    accuracies = []   #[train_accuracy valid_accuracy]\n",
        "    accuracies_path = fold_path + 'accuracies.npy'\n",
        "\n",
        "    if CONTINUE[fold] > 1:\n",
        "        model.load_state_dict(torch.load(fold_path + 'epoch_' + str(CONTINUE[fold]) + '.pth'))\n",
        "        print('Weight Loaded!')\n",
        "        training_losses = list(np.load(tracking_losses_path))[:CONTINUE[fold]]\n",
        "        accuracies = list(np.load(accuracies_path))[:CONTINUE[fold]]\n",
        "        print(f'Fold {fold} Parameter Loaded!')\n",
        "\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_subsampler, num_workers=0)\n",
        "    valid_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=valid_subsampler, num_workers=0)\n",
        "\n",
        "    print(f\"Number of batches in train_dataloader: {len(train_dataloader)}\")\n",
        "\n",
        "    # Test accessing the first batch\n",
        "    current_images, goal_images, labels = next(iter(train_dataloader))\n",
        "    print(f\"Batch shapes: {current_images.shape}, {goal_images.shape}, {labels.shape}\")\n",
        "\n",
        "\n",
        "    # Train Model\n",
        "    model.train()\n",
        "    while training_loss > ((TOLERANCE ** 2) * LOSS_SCALE):\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for current_images, goal_images, labels in train_dataloader:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(current_images, goal_images)\n",
        "            # print(output.flatten())\n",
        "            # print(labels.float())\n",
        "            loss = loss_fn(output.flatten(), labels.float()) * LOSS_SCALE\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        training_loss = running_loss / len(train_dataloader)\n",
        "\n",
        "        # Moving Average\n",
        "        training_total_loss += training_loss * 5\n",
        "        training_average_loss = training_total_loss / (len(training_losses) + 5)\n",
        "        training_total_loss = training_average_loss * (len(training_losses) + 1)\n",
        "\n",
        "        # Save training loss\n",
        "        training_losses.append([training_loss, training_average_loss])\n",
        "        print(f'Epoch {epoch}, Loss: {training_losses[epoch - 1][0]:.6f}, Average Loss: {training_losses[epoch - 1][1]:.6f}', end='; ')\n",
        "        np.save(tracking_losses_path, training_losses)\n",
        "\n",
        "        if (epoch % WEIGHT_SAVING_STEP) == 0:\n",
        "            torch.save(model.state_dict(), (fold_path + 'epoch_' + str(epoch) + '.pth'))\n",
        "            print('Save Weights', end='; ')\n",
        "\n",
        "        # Valid Model\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "\n",
        "            num_correct, num_total = 0, 0\n",
        "            for current_images, goal_images, labels in train_dataloader:\n",
        "                output = model(current_images, goal_images)\n",
        "                for i in range(len(output)):\n",
        "                    loss = abs(output[i] - labels[i]).item()\n",
        "                    num_total += 1\n",
        "                    if loss < TOLERANCE:\n",
        "                        num_correct += 1\n",
        "            train_accuracy = (num_correct / num_total) * 100\n",
        "\n",
        "            num_correct, num_total = 0, 0\n",
        "            for current_images, goal_images, labels in valid_dataloader:\n",
        "                output = model(current_images, goal_images)\n",
        "                for i in range(len(output)):\n",
        "                    loss = abs(output[i] - labels[i]).item()\n",
        "                    num_total += 1\n",
        "                    if loss < TOLERANCE:\n",
        "                        num_correct += 1\n",
        "            valid_accuracy = (num_correct / num_total) * 100\n",
        "\n",
        "            accuracies.append([train_accuracy, valid_accuracy])\n",
        "            print(f'Train Accuracy {accuracies[epoch - 1][0]:.2f}%, Valid Accuracy: {accuracies[epoch - 1][1]:.2f}%')\n",
        "            np.save(accuracies_path, accuracies)\n",
        "\n",
        "            epoch += 1\n",
        "\n",
        "    print(f'Finished Training fold {fold}')\n",
        "    epoch -= 1\n",
        "\n",
        "    # Save last weight\n",
        "    torch.save(model.state_dict(), (WEIGHT_PATH + 'fold_' + str(fold) + '/epoch_' + str(epoch) + '.pth'))\n",
        "    print('Save Last Weights')\n",
        "\n",
        "    # Plot Training Loss and Accuracies graphs\n",
        "    plot_graph(training_losses, accuracies, FIGURE_PATH, fold, end_plot=epoch)"
      ],
      "metadata": {
        "id": "cNb7EJ_8EI8Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "44d23568-ebd8-4be8-a7c0-3a5d2e5bc59c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom torch.utils.data import DataLoader\\nimport torch, os\\nfrom torchvision import transforms\\nimport numpy as np\\nfrom sklearn.model_selection import KFold\\n\\n# Setup Destination\\nDATASET_NAME = \\'mixed\\'\\nDATASET_INITIAL_PATH = \\'/content/drive/MyDrive/Spot_IL/CLIPSeg_Mixed_Dataset_Train_Test\\'\\nWEIGHT_FOLDER_NAME = \\'lr1e-5_with_scaling\\'\\n\\n# Paths\\nTRAIN_PATH = os.path.join(DATASET_INITIAL_PATH, \\'train/\\')\\nTEST_PATH = os.path.join(DATASET_INITIAL_PATH, \\'test/\\')\\nGOAL_PATH = os.path.join(DATASET_INITIAL_PATH, \\'goal/goal_images/\\')\\nLABEL_PATH = os.path.join(TRAIN_PATH, \\'labels_radians.npy\\')\\n\\nprint(TRAIN_PATH)\\nprint(TEST_PATH)\\nprint(GOAL_PATH)\\nprint(LABEL_PATH)\\n\\n# Output Paths\\nWEIGHT_PATH = os.path.join(DATASET_INITIAL_PATH,\\'train\\', f\\'weights/Train_FiveResNet18MLP5_{DATASET_NAME}\\', WEIGHT_FOLDER_NAME)\\nFIGURE_PATH = os.path.join(DATASET_INITIAL_PATH, \\'train\\',f\\'Results/Train_FiveResNet18MLP5_{DATASET_NAME}\\', WEIGHT_FOLDER_NAME)\\n\\n# Ensure directories exist\\nos.makedirs(WEIGHT_PATH, exist_ok=True)\\nos.makedirs(FIGURE_PATH, exist_ok=True)\\n\\n# KFold Parameters\\nNUM_FOLD = 5\\nk_fold = KFold(NUM_FOLD, shuffle=True)\\nCONTINUE = [0] * NUM_FOLD   # Start from beginning, use 0\\n\\n# Preprocess for images\\ndata_transforms = transforms.Compose([\\n    transforms.Resize((224, 224)),\\n    transforms.ToTensor(),\\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\\n])\\n\\nif torch.cuda.is_available():\\n\\n    train_dataset = SPOTDataLoader(\\n        root_dir = TRAIN_PATH,\\n        goal_folder = GOAL_PATH,\\n        labels_file = LABEL_PATH,\\n        transform = data_transforms\\n    )\\n    DEVICE = \\'cuda\\'\\n    print(\\'Cuda\\')\\n\\nelse:\\n    train_dataset = SPOTDataLoader(\\n        root_dir = TRAIN_PATH,\\n        goal_folder = GOAL_PATH,\\n        labels_file = LABEL_PATH,\\n        transform = data_transforms\\n    )\\n    DEVICE = \\'cpu\\'\\n    print(\\'CPU\\')\\n\\n# Hyper Parameters\\nloss_fn = torch.nn.MSELoss()\\nBATCH_SIZE = 2\\nLEARNING_RATE = 1e-5\\n\\n# Training Parameters\\nWEIGHT_SAVING_STEP = 10\\nLOSS_SCALE = 1e3\\n\\n# Validation Parameter\\nTOLERANCE = 1e-4\\n\\n# Saving Hyper Param\\nhyper_params_path = WEIGHT_PATH + \\'hyper_params\\'\\nhyper_params = {\\'NUM_FOLD\\': NUM_FOLD, \\'BATCH_SIZE\\': BATCH_SIZE, \\'LEARNING_RATE\\': LEARNING_RATE, \\'LOSS_SCALE\\': LOSS_SCALE, \\'TOLERANCE\\': TOLERANCE}\\nnp.savez(hyper_params_path, **hyper_params)\\n\\nfor fold, (train_ids, valid_ids) in enumerate(k_fold.split(train_dataset)):\\n    if fold < 2:\\n        continue\\n\\n    print(f\\'FOLD {fold}\\')\\n    fold_path = WEIGHT_PATH + \\'fold_\\' + str(fold) + \\'/\\'\\n    if not os.path.exists(fold_path):\\n        os.mkdir(fold_path)\\n\\n    # Setup Model\\n    model = FiveResNet18MLP5().to(DEVICE)\\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\\n\\n    # Tracking Parameters\\n    epoch = CONTINUE[fold] + 1\\n    training_loss = 1e6\\n    training_total_loss = 0\\n    training_losses = []   #[training_loss training_average_loss]\\n    tracking_losses_path = fold_path + \\'training_losses.npy\\'\\n    accuracies = []   #[train_accuracy valid_accuracy]\\n    accuracies_path = fold_path + \\'accuracies.npy\\'\\n\\n    if CONTINUE[fold] > 1:\\n        model.load_state_dict(torch.load(fold_path + \\'epoch_\\' + str(CONTINUE[fold]) + \\'.pth\\'))\\n        print(\\'Weight Loaded!\\')\\n        training_losses = list(np.load(tracking_losses_path))[:CONTINUE[fold]]\\n        accuracies = list(np.load(accuracies_path))[:CONTINUE[fold]]\\n        print(f\\'Fold {fold} Parameter Loaded!\\')\\n\\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\\n    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\\n\\n    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_subsampler, num_workers=0)\\n    valid_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=valid_subsampler, num_workers=0)\\n\\n    print(f\"Number of batches in train_dataloader: {len(train_dataloader)}\")\\n\\n    # Test accessing the first batch\\n    current_images, goal_images, labels = next(iter(train_dataloader))\\n    print(f\"Batch shapes: {current_images.shape}, {goal_images.shape}, {labels.shape}\")\\n\\n\\n    # Train Model\\n    model.train()\\n    while training_loss > ((TOLERANCE ** 2) * LOSS_SCALE):\\n\\n        running_loss = 0.0\\n\\n        for current_images, goal_images, labels in train_dataloader:\\n\\n            optimizer.zero_grad()\\n            output = model(current_images, goal_images)\\n            # print(output.flatten())\\n            # print(labels.float())\\n            loss = loss_fn(output.flatten(), labels.float()) * LOSS_SCALE\\n\\n            loss.backward()\\n            optimizer.step()\\n            running_loss += loss.item()\\n\\n        training_loss = running_loss / len(train_dataloader)\\n\\n        # Moving Average\\n        training_total_loss += training_loss * 5\\n        training_average_loss = training_total_loss / (len(training_losses) + 5)\\n        training_total_loss = training_average_loss * (len(training_losses) + 1)\\n\\n        # Save training loss\\n        training_losses.append([training_loss, training_average_loss])\\n        print(f\\'Epoch {epoch}, Loss: {training_losses[epoch - 1][0]:.6f}, Average Loss: {training_losses[epoch - 1][1]:.6f}\\', end=\\'; \\')\\n        np.save(tracking_losses_path, training_losses)\\n\\n        if (epoch % WEIGHT_SAVING_STEP) == 0:\\n            torch.save(model.state_dict(), (fold_path + \\'epoch_\\' + str(epoch) + \\'.pth\\'))\\n            print(\\'Save Weights\\', end=\\'; \\')\\n\\n        # Valid Model\\n        model.eval()\\n        with torch.no_grad():\\n\\n            num_correct, num_total = 0, 0\\n            for current_images, goal_images, labels in train_dataloader:\\n                output = model(current_images, goal_images)\\n                for i in range(len(output)):\\n                    loss = abs(output[i] - labels[i]).item()\\n                    num_total += 1\\n                    if loss < TOLERANCE:\\n                        num_correct += 1\\n            train_accuracy = (num_correct / num_total) * 100\\n\\n            num_correct, num_total = 0, 0\\n            for current_images, goal_images, labels in valid_dataloader:\\n                output = model(current_images, goal_images)\\n                for i in range(len(output)):\\n                    loss = abs(output[i] - labels[i]).item()\\n                    num_total += 1\\n                    if loss < TOLERANCE:\\n                        num_correct += 1\\n            valid_accuracy = (num_correct / num_total) * 100\\n\\n            accuracies.append([train_accuracy, valid_accuracy])\\n            print(f\\'Train Accuracy {accuracies[epoch - 1][0]:.2f}%, Valid Accuracy: {accuracies[epoch - 1][1]:.2f}%\\')\\n            np.save(accuracies_path, accuracies)\\n\\n            epoch += 1\\n\\n    print(f\\'Finished Training fold {fold}\\')\\n    epoch -= 1\\n\\n    # Save last weight\\n    torch.save(model.state_dict(), (WEIGHT_PATH + \\'fold_\\' + str(fold) + \\'/epoch_\\' + str(epoch) + \\'.pth\\'))\\n    print(\\'Save Last Weights\\')\\n\\n    # Plot Training Loss and Accuracies graphs\\n    plot_graph(training_losses, accuracies, FIGURE_PATH, fold, end_plot=epoch)\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch, os\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters\n",
        "WEIGHT_DATASET_NAME = 'mixed'\n",
        "TEST_DATASET_NAME = 'mixed'\n",
        "TEST_DATASET_TYPE = 'test'\n",
        "FOLD = 0\n",
        "WEIGHT_NAME = 'epoch_297.pth'\n",
        "\n",
        "# Constants\n",
        "DATASET_PATH = '/content/drive/MyDrive/Spot_IL/CLIPSeg_Mixed_Dataset_Train_Test'\n",
        "TEST_PATH = os.path.join(DATASET_PATH, TEST_DATASET_TYPE)\n",
        "GOAL_PATH = os.path.join(DATASET_PATH, 'goal', 'goal_images')\n",
        "LABEL_PATH = os.path.join(TEST_PATH, 'labels_radians.npy')\n",
        "\n",
        "# Output Paths\n",
        "WEIGHT_PATH = os.path.join(DATASET_PATH, 'test', 'weights', f'FiveResNet18MLP5_{WEIGHT_DATASET_NAME}', 'lr1e-5_with_scaling', f'fold_{FOLD}')\n",
        "FIGURES_PATH = os.path.join(DATASET_PATH, 'test', 'Results', f'FiveResNet18MLP5_{WEIGHT_DATASET_NAME}', 'lr1e-5_with_scaling', f'fold_{FOLD}')\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(WEIGHT_PATH, exist_ok=True)\n",
        "os.makedirs(FIGURES_PATH, exist_ok=True)\n",
        "\n",
        "DPI = 120\n",
        "FIGURE_SIZE_PIXEL = [2490, 1490]\n",
        "FIGURE_SIZE = [fsp / DPI for fsp in FIGURE_SIZE_PIXEL]\n",
        "\n",
        "def test_model(test_dataset, model, weight_name, device='cuda', draw=False, show=False):\n",
        "\n",
        "    model.to(device)\n",
        "    # model.load_state_dict(torch.load(os.path.join(WEIGHT_PATH, weight_name)))\n",
        "    model.eval()\n",
        "\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
        "    results = np.empty([0, 3])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        idx = 0\n",
        "        for current_images, goal_images, label in test_dataloader:\n",
        "            output = model(current_images, goal_images)\n",
        "            output_degree = (output.item() / np.pi) * 180\n",
        "            label_degree = (label.item() / np.pi) * 180\n",
        "            loss = abs(label - output)\n",
        "\n",
        "            iteration_result = np.array([output_degree, label_degree, loss.item()])\n",
        "            results = np.vstack([results, iteration_result])\n",
        "\n",
        "            print(idx, iteration_result)\n",
        "            idx += 1\n",
        "\n",
        "    # Plot\n",
        "    if draw is True:\n",
        "        plt.figure(figsize=FIGURE_SIZE, dpi=DPI)\n",
        "        plt.plot(range(len(test_dataloader)), results[:, 0], color='green', linestyle='-', label='Predicted Rotation Angle')\n",
        "        plt.plot(range(len(test_dataloader)), results[:, 1], color='cyan', linestyle='-', label='GT Rotation Angle')\n",
        "        plt.plot(range(len(test_dataloader)), results[:, 2], color='blue', linestyle='-', label='Difference')\n",
        "        plt.title(f'Test for {weight_name}')\n",
        "        plt.xlabel(\"Datapoint\")\n",
        "        plt.ylabel(\"Degree\")\n",
        "        plt.legend()\n",
        "\n",
        "        if show is True:\n",
        "            plt.show()\n",
        "        else:\n",
        "            file_name = FIGURES_PATH + f'{weight_name.split(\".pth\")[0]}_test_with_{TEST_DATASET_NAME}_{TEST_DATASET_TYPE}'\n",
        "            plt.savefig(file_name + '.png')\n",
        "            plt.close()\n",
        "\n",
        "            np.savetxt(file_name + '.csv', results, delimiter=',')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    model = FiveResNet18MLP5()\n",
        "\n",
        "    # Preprocess for images\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "\n",
        "        test_dataset = SPOTDataLoader(\n",
        "            root_dir = TEST_PATH,\n",
        "            goal_folder = GOAL_PATH,\n",
        "            labels_file = LABEL_PATH,\n",
        "            transform = data_transforms\n",
        "        )\n",
        "        DEVICE = 'cuda'\n",
        "        print('Cuda')\n",
        "\n",
        "    else:\n",
        "        test_dataset = SPOTDataLoader(\n",
        "            root_dir = TEST_PATH,\n",
        "            goal_folder = GOAL_PATH,\n",
        "            labels_file = LABEL_PATH,\n",
        "            transform = data_transforms\n",
        "        )\n",
        "        DEVICE = 'cpu'\n",
        "        print('CPU')\n",
        "\n",
        "    weight_name = WEIGHT_NAME\n",
        "    test_model(test_dataset, model, weight_name, device=DEVICE, draw=True, show=False)"
      ],
      "metadata": {
        "id": "_HkRgZvtD5L5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a703fd5-a241-4249-e991-a6990d8ca652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda\n",
            "0 [0.33789298 0.97402825 0.01110265]\n",
            "1 [ 0.52705747 -0.97402825  0.02619889]\n",
            "2 [ 0.32813896 -0.97402825  0.02272711]\n",
            "3 [ 0.54641215 -0.97402825  0.02653669]\n",
            "4 [0.61098438 0.97402825 0.00633631]\n",
            "5 [ 0.37340716 -0.97402825  0.02351718]\n",
            "6 [ 0.10473634 -0.97402825  0.01882799]\n",
            "7 [ 0.48596785 -0.97402825  0.02548174]\n",
            "8 [0.44158646 0.97402825 0.00929286]\n",
            "9 [0.36719823 0.97402825 0.01059118]\n",
            "10 [0.56791438 0.97402825 0.00708802]\n",
            "11 [0.44859407 0.97402825 0.00917056]\n",
            "12 [0.50297646 0.97402825 0.0082214 ]\n",
            "13 [0.1888681  0.97402825 0.01370363]\n",
            "14 [ 0.29718142 -0.97402825  0.02218679]\n",
            "15 [ 0.27508181 -0.97402825  0.02180108]\n",
            "16 [ 0.23554014 -0.97402825  0.02111095]\n",
            "17 [ 0.53648804 -0.97402825  0.02636348]\n",
            "18 [0.45839577 0.97402825 0.00899948]\n",
            "19 [ 0.06939491 -0.97402825  0.01821117]\n",
            "20 [0.17662015 0.97402825 0.0139174 ]\n",
            "21 [0.54186276 0.97402825 0.00754271]\n",
            "22 [0.49429375 0.97402825 0.00837295]\n",
            "23 [ 0.54661897 -0.97402825  0.0265403 ]\n",
            "24 [ 0.3117992  -0.97402825  0.02244192]\n",
            "25 [0.52706329 0.97402825 0.00780101]\n",
            "26 [0.71559614 0.97402825 0.00451049]\n",
            "27 [0.46537473 0.97402825 0.00887768]\n",
            "28 [0.64024155 0.97402825 0.00582568]\n",
            "29 [0.50872989 0.97402825 0.00812099]\n",
            "30 [0.3974331  0.97402825 0.01006348]\n",
            "31 [0.30543931 0.97402825 0.01166908]\n",
            "32 [0.59641356 0.97402825 0.00659062]\n",
            "33 [ 0.1730235  -0.97402825  0.02001983]\n",
            "34 [ 0.22544373 -0.97402825  0.02093474]\n",
            "35 [0.44995483 0.97402825 0.00914681]\n",
            "36 [0.63427138 0.97402825 0.00592988]\n",
            "37 [ 0.46999572 -0.97402825  0.02520297]\n",
            "38 [0.31035382 0.97402825 0.0115833 ]\n",
            "39 [0.58197566 0.97402825 0.00684261]\n",
            "40 [0.3827987  0.97402825 0.0103189 ]\n",
            "41 [0.33941774 0.97402825 0.01107604]\n",
            "42 [0.6959847  0.97402825 0.00485278]\n",
            "43 [0.55327713 0.97402825 0.00734349]\n",
            "44 [0.25555289 0.97402825 0.01253976]\n",
            "45 [ 0.34586923 -0.97402825  0.02303656]\n",
            "46 [0.37402145 0.97402825 0.01047209]\n",
            "47 [ 0.55123175 -0.97402825  0.02662081]\n",
            "48 [0.37861446 0.97402825 0.01039193]\n",
            "49 [ 0.294374   -0.97402825  0.0221378 ]\n",
            "50 [ 0.33908682 -0.97402825  0.02291818]\n",
            "51 [0.40064726 0.97402825 0.01000739]\n",
            "52 [ 0.49636596 -0.97402825  0.02566322]\n",
            "53 [ 0.53003111 -0.97402825  0.02625079]\n",
            "54 [ 0.43691482 -0.97402825  0.0246256 ]\n",
            "55 [ 0.47628798 -0.97402825  0.02531279]\n",
            "56 [ 0.42196359 -0.97402825  0.02436465]\n",
            "57 [0.63290224 0.97402825 0.00595377]\n",
            "58 [ 0.57386801 -0.97402825  0.02701589]\n",
            "59 [ 0.32783843 -0.97402825  0.02272186]\n",
            "60 [ 0.55454204 -0.97402825  0.02667858]\n",
            "61 [ 0.35007681 -0.97402825  0.02310999]\n",
            "62 [ 0.30981674 -0.97402825  0.02240732]\n",
            "63 [0.39932802 0.97402825 0.01003041]\n",
            "64 [0.36312439 0.97402825 0.01066228]\n",
            "65 [0.32562028 0.97402825 0.01131685]\n",
            "66 [ 0.18955645 -0.97402825  0.02030838]\n",
            "67 [0.30081524 0.97402825 0.01174978]\n",
            "68 [ 0.33104998 -0.97402825  0.02277791]\n",
            "69 [ 0.12856236 -0.97402825  0.01924384]\n",
            "70 [0.5667532  0.97402825 0.00710829]\n",
            "71 [0.32086009 0.97402825 0.01139994]\n",
            "72 [0.50039951 0.97402825 0.00826638]\n",
            "73 [0.3633428  0.97402825 0.01065847]\n",
            "74 [ 0.50945378 -0.97402825  0.02589165]\n",
            "75 [0.37376479 0.97402825 0.01047657]\n",
            "76 [0.38212961 0.97402825 0.01033058]\n",
            "77 [ 0.21435711 -0.97402825  0.02074124]\n",
            "78 [0.68056458 0.97402825 0.00512191]\n",
            "79 [ 0.2476438  -0.97402825  0.0213222 ]\n",
            "80 [ 0.48659516 -0.97402825  0.02549269]\n",
            "81 [ 0.2652619  -0.97402825  0.02162969]\n",
            "82 [ 0.27695598 -0.97402825  0.02183379]\n",
            "83 [ 0.53223673 -0.97402825  0.02628928]\n",
            "84 [0.56987427 0.97402825 0.00705382]\n",
            "85 [0.43108883 0.97402825 0.00947608]\n",
            "86 [ 0.28933351 -0.97402825  0.02204982]\n",
            "87 [ 0.3621623  -0.97402825  0.02332092]\n",
            "88 [ 0.10707159 -0.97402825  0.01886875]\n",
            "89 [ 0.35140448 -0.97402825  0.02313317]\n",
            "90 [ 0.12399237 -0.97402825  0.01916408]\n",
            "91 [ 0.5574493  -0.97402825  0.02672933]\n",
            "92 [0.31298848 0.97402825 0.01153732]\n",
            "93 [0.57970206 0.97402825 0.00688229]\n",
            "94 [0.0503327  0.97402825 0.01612153]\n",
            "95 [0.45616907 0.97402825 0.00903835]\n",
            "96 [ 0.31271882 -0.97402825  0.02245797]\n",
            "97 [ 0.53275134 -0.97402825  0.02629826]\n",
            "98 [0.03791462 0.97402825 0.01633827]\n",
            "99 [ 0.49284703 -0.97402825  0.0256018 ]\n",
            "100 [ 0.12225377 -0.97402825  0.01913373]\n",
            "101 [0.55745048 0.97402825 0.00727065]\n",
            "102 [ 0.48426031 -0.97402825  0.02545194]\n",
            "103 [0.62502015 0.97402825 0.00609134]\n",
            "104 [0.66345405 0.97402825 0.00542054]\n",
            "105 [ 0.55912996 -0.97402825  0.02675866]\n",
            "106 [0.61893285 0.97402825 0.00619758]\n",
            "107 [ 0.28564353 -0.97402825  0.02198542]\n",
            "108 [0.55851353 0.97402825 0.0072521 ]\n",
            "109 [ 0.13561909 -0.97402825  0.019367  ]\n",
            "110 [ 0.52245648 -0.97402825  0.02611859]\n",
            "111 [0.25414928 0.97402825 0.01256426]\n",
            "112 [ 0.51691571 -0.97402825  0.02602188]\n"
          ]
        }
      ]
    }
  ]
}